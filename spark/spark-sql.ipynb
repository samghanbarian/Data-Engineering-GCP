{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b300147",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session\n",
    "spark = SparkSession.builder\\\n",
    ".master('loacal[*]')\\\n",
    ".appName('ny_taxi')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read grren and yellow dataset into daatframe\n",
    "df_green = spark.read.parquet('data/green/*/*')\n",
    "df_yellow = spark.read.parquet('data/yellow/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c51ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare to join the two dfs\n",
    "set(df_green.columns) & set(df_yellow.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e8874",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it is not showing the dropoff and pickup datetime cols. so we should unify the name\n",
    "df_green\\\n",
    ".withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime')\\\n",
    ".withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "df_yellow\\\n",
    ".withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime')\\\n",
    ".withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4292122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a137c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = []\n",
    "yellow_cols = set(df_yellow.columns)\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_cols:\n",
    "        common_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190487e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select common columns in both datasets adding extra column for taxi color\n",
    "df_selected_y = df_yellow.select(common_cols).withColumn('taxi_type',F.lit('yellow'))\n",
    "df_selected_g = df_green.select(common_cols).withColumn('taxi_type',F.lit('green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df = df_selected_y.unionAll(df_selected_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74997d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df.groupBy('taxi_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd21370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e265978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be able to run query on df we need to register it as a temporary table\n",
    "trip_df.registerTempTable('trip_tb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e70c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6885f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql =spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    taxi_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_montly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_montly_trip_distance\n",
    "FROM\n",
    "    trip_tb\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the resulta to parquet\n",
    "df_sql.write.parquet('data/reports/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use coalesce to reduce the number of partitions\n",
    "df_sql.coalesce(|).write.parquet('data/reports',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009c009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
